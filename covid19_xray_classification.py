# -*- coding: utf-8 -*-
"""covid19 Xray classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zh0j7Hw9o06KV3exihSYFXDcZWcZeLj9

Project ini menggunakan dataset image classification yang membandingkan hasil x-ray pasien covid dengan pasien normal dan pasien yang berpotensi mengalami pneumonia.

Dalam dataset tersebut ada 3 folder yaitu CovidDataset, Fuzzy, dan stacked,
dimana folder CovidDataset berisi gambar hasil x-ray yang belum dilakukan proses pre-processing.

Sedangkan folder fuzzy adalah folder yang berisi gambar yang sudah dilakukan pre-processing gambar dengan teknik pengurangan noise, sedangkan folder Stacked berisi file gambar yang ditumpuk antara gambar asli dan gambar yang sudah di pre-processing.


Sedangkan untuk model ini saya akan menggabungkan semua dataset tersebut kedalam 1 folder CovidDataset dan akan saya bagi antara train dan validation dengan pembagian 80% & 20%.

link dataset : https://www.kaggle.com/shreyanshgupta/covid19-xray-dataset-with-preprocessed-images
"""

# menghubungakan dengan google drive

from google.colab import drive
drive.mount('/content/drive/')

import os

base_dir = '/content/drive/MyDrive/CovidDataset'
print(os.listdir(base_dir))

# Split data train and data validation
!pip install split-folders
import splitfolders

splitfolders.ratio('/content/drive/MyDrive/CovidDataset', 
                   output="/content/drive/MyDrive/CovidDataset/split", seed=1337, ratio=(0.8, 0.2))

# Make train directory and validation directory

split_dir = "/content/drive/MyDrive/CovidDataset/split"
train_dir = os.path.join(split_dir,'train')
val_dir = os.path.join(split_dir, 'val')

# Set Training directory
train_covid = os.path.join(train_dir, 'Covid')
train_normal = os.path.join(train_dir, 'Normal')
train_pneumonia = os.path.join(train_dir, 'Pneumonia')

# Set Validation directory
val_covid = os.path.join(val_dir, 'Covid')
val_normal = os.path.join(val_dir, 'Normal')
val_pneumonia = os.path.join(val_dir, 'Pneumonia')

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Generate Image
train_datagen = ImageDataGenerator(
                    rescale=1./255,
                    rotation_range=20,
                    horizontal_flip=True,
                    shear_range = 0.2,
                    fill_mode = 'nearest')

val_datagen = ImageDataGenerator(
                    rescale=1./255,
                    rotation_range=20,
                    horizontal_flip=True,
                    shear_range = 0.2,
                    fill_mode = 'nearest')

train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(150,150),
    batch_size= 128,
    class_mode='categorical'
)

val_generator = val_datagen.flow_from_directory(
    val_dir,
    target_size = (150,150),
    batch_size = 128,
    class_mode = 'categorical'
)

# Setup Model
model = tf.keras.models.Sequential([
  tf.keras.layers.Conv2D(32, (3,3), activation = 'relu', input_shape= (150,150,3)),
  tf.keras.layers.MaxPooling2D(2,2),
  tf.keras.layers.Conv2D(64,(3,3), activation= 'relu'),
  tf.keras.layers.MaxPooling2D(2,2),
  tf.keras.layers.Conv2D(128,(3,3), activation= 'relu'),
  tf.keras.layers.MaxPooling2D(2,2),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(512, activation= 'relu'),
  tf.keras.layers.Dense(3, activation= 'softmax')
])

model.summary()
model.compile(loss='categorical_crossentropy',
              optimizer=tf.optimizers.Adam(),
              metrics=['accuracy'])

# # Multi-Class Cross-Entropy Loss
# from keras.optimizers import SGD
# opt = SGD(lr=0.01, momentum=0.9)
# model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])

class myCallback(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs={}):
    if(logs.get('accuracy')>0.92 and logs.get('val_accuracy')>0.92):
      print("\nAkurasi train dan validasi sudah mencapai terget > 92%!")
      self.model.stop_training = True
callbacks = myCallback()

# callbacks_modelCheckpoint = tf.keras.callbacks.ModelCheckpoint(filepath = 'model.h5', verbose = 1, save_best_only = True)

checkpoint_filepath = '/tmp/checkpoint'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    verbose=1,
    save_best_only=True)

batch_size = 128

history = model.fit(
    train_generator,
    steps_per_epoch = 2610/batch_size,
    epochs = 20,
    validation_data = val_generator,
    validation_steps = 654/batch_size,
    verbose =2,
    callbacks=[callbacks, model_checkpoint_callback]
)

# load checkpoint

# model.load_weights(checkpoint_filepath)

# save model
model.save(checkpoint_filepath)

# Loss training and validation
from matplotlib import pyplot as plt

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Loss Plot')
plt.ylabel('Value')
plt.xlabel('Epoch')
plt.legend(loc="upper right")
plt.show()

# Accuracy training & validation
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy Plot')
plt.ylabel('Value')
plt.xlabel('Epoch')
plt.legend(loc="lower right")
plt.show()

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
from google.colab import files
from keras.preprocessing import image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
# %matplotlib inline

uploaded = files.upload()

for fn in uploaded.keys():

  path = fn 
  img = image.load_img(path, target_size =(150,150))
  imgplot = plt.imshow(img)
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)

  images = np.vstack([x])
  classes = model.predict(images, batch_size=10)

  print(fn)
  if classes[0,0]!=0:
    print('Covid')
  elif classes[0,1]!=0:
    print('Normal')
  else:
    print('Pneumonia')

# Converting a SavedModel to a TensorFlow Lite model.
converter = tf.lite.TFLiteConverter.from_saved_model(checkpoint_filepath)
tflite_model = converter.convert()
            
# Save the model.
with open('model.tflite', 'wb') as f:
  f.write(tflite_model)